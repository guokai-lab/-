![image](https://github.com/guokai-lab/-/blob/main/image.png)
梯度消失 (Vanishing Gradients)
  现象: 在反向传播过程中，梯度值变得非常小，甚至接近于零。
  影响: 导致神经网络的浅层（靠近输入层）参数更新缓慢甚至停滞，使得这些层无法有效地学习到特征，从而影响整个网络的学习能力。模型可能无法捕捉到长期的依赖关系。

梯度爆炸 (Exploding Gradients)
  现象: 在反向传播过程中，梯度值变得非常大，甚至趋于无穷大。
  影响: 导致模型参数更新过大，使得训练过程变得非常不稳定，损失函数值剧烈震荡，甚至出现 NaN (Not a Number) 或 inf (Infinity) 值，导致训练崩溃。

解决办法：
  1.ReLU激活函数
  2.梯度裁剪
  3.批归一化（Batch Normalization）
  4.残差网络
